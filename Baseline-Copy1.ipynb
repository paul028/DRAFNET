{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Necessary Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from haversine_script import *\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import pandas as p\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout,Activation,BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import Callback, TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exponential_distance(x,minimum,a=60):\n",
    "\tpositive_x= x-minimum\n",
    "\tnumerator = np.exp(positive_x.div(a))\n",
    "\tdenominator = np.exp(-minimum/a)\n",
    "\texponential_x = numerator/denominator\n",
    "\texponential_x = exponential_x * 1000  #facilitating calculations\n",
    "\tfinal_x = exponential_x\n",
    "\treturn final_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_powed_distance(x,minimum,b=1.1):\n",
    "\tpositive_x= x-minimum\n",
    "\tnumerator = positive_x.pow(b)\n",
    "\tdenominator = (-minimum)**(b)\n",
    "\tpowed_x = numerator/denominator\n",
    "\tfinal_x = powed_x\n",
    "\treturn final_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Random Seeding for experiment reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED'] = \"42\"\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1) \n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "file = p.read_csv('lorawan_antwerp_2019_dataset.csv')\n",
    "columns = file.columns\n",
    "# x = file[columns[0:68]]\n",
    "# y = file[columns[71:]]\n",
    "x = file[columns[0:72]]\n",
    "x = x.join(file[columns[73]])\n",
    "y = file[columns[72:]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum\n",
      "-128.0\n"
     ]
    }
   ],
   "source": [
    "x = x.replace(-200,200)\n",
    "minimum = x.min().min() - 1\n",
    "x = x.replace(200,minimum)\n",
    "print('minimum')\n",
    "print(minimum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RSSI Data representation using Powed Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_x = get_powed_distance(x,minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91300, 73)\n",
      "(19564, 73)\n",
      "(19565, 73)\n"
     ]
    }
   ],
   "source": [
    "random_state = 42\n",
    "x_train, x_test_val, y_train, y_test_val = train_test_split(final_x.values, y.values, test_size=0.3, random_state=random_state)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test_val, y_test_val, test_size=0.5, random_state=random_state)\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dataset Normalization [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_features = x_train.shape[1]\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "scaler_y = preprocessing.MinMaxScaler().fit(y_train)\n",
    "y_train = scaler_y.transform(y_train)\n",
    "y_val = scaler_y.transform(y_val)\n",
    "y_test = scaler_y.transform(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.15\n",
    "l2 = 0.00\n",
    "lr = 0.0005\n",
    "epochs = 300\n",
    "batch_size= 512\n",
    "patience = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the MLP Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1125 16:14:29.141091 27220 deprecation_wrapper.py:119] From C:\\Users\\Paul Vincent Nonat\\AppData\\Local\\conda\\conda\\envs\\nonat_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1125 16:14:29.163065 27220 deprecation_wrapper.py:119] From C:\\Users\\Paul Vincent Nonat\\AppData\\Local\\conda\\conda\\envs\\nonat_gpu\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#model.add(Dense(units=1024, input_dim=n_of_features, kernel_regularizer=regularizers.l2(l2)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(dropout, seed=random_state))\n",
    "#model.add(Dense(units=1024, input_dim=n_of_features, kernel_regularizer=regularizers.l2(l2)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(dropout, seed=random_state))\n",
    "#model.add(Dense(units=1024, input_dim=n_of_features, kernel_regularizer=regularizers.l2(l2)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(dropout, seed=random_state))\n",
    "#model.add(Dense(units=256, kernel_regularizer=regularizers.l2(l2)))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(dropout, seed=random_state))\n",
    "model.add(Dense(units=128, kernel_regularizer=regularizers.l2(l2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Dropout(dropout, seed=random_state))\n",
    "model.add(Dense(units=128, kernel_regularizer=regularizers.l2(l2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "# model.add(Dropout(dropout))\n",
    "model.add(Dense(units=2))\n",
    "model.compile(loss='mean_absolute_error',optimizer=Adam(lr=lr))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1125 16:14:33.078069 27220 deprecation_wrapper.py:119] From C:\\Users\\Paul Vincent Nonat\\AppData\\Local\\conda\\conda\\envs\\nonat_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1125 16:14:33.080064 27220 deprecation_wrapper.py:119] From C:\\Users\\Paul Vincent Nonat\\AppData\\Local\\conda\\conda\\envs\\nonat_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1125 16:14:33.180794 27220 deprecation_wrapper.py:119] From C:\\Users\\Paul Vincent Nonat\\AppData\\Local\\conda\\conda\\envs\\nonat_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W1125 16:14:33.617626 27220 deprecation_wrapper.py:119] From C:\\Users\\Paul Vincent Nonat\\AppData\\Local\\conda\\conda\\envs\\nonat_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 91300 samples, validate on 19564 samples\n",
      "Epoch 1/300\n",
      "91300/91300 [==============================] - 3s 34us/step - loss: 0.1933 - val_loss: 0.1052\n",
      "Epoch 2/300\n",
      "91300/91300 [==============================] - 2s 24us/step - loss: 0.0726 - val_loss: 0.0725\n",
      "Epoch 3/300\n",
      "91300/91300 [==============================] - ETA: 0s - loss: 0.059 - 2s 26us/step - loss: 0.0594 - val_loss: 0.0613\n",
      "Epoch 4/300\n",
      "91300/91300 [==============================] - 2s 25us/step - loss: 0.0530 - val_loss: 0.0536\n",
      "Epoch 5/300\n",
      "91300/91300 [==============================] - 2s 24us/step - loss: 0.0487 - val_loss: 0.0482\n",
      "Epoch 6/300\n",
      "91300/91300 [==============================] - 2s 25us/step - loss: 0.0439 - val_loss: 0.0458\n",
      "Epoch 7/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0420 - val_loss: 0.0380\n",
      "Epoch 8/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0399 - val_loss: 0.0365\n",
      "Epoch 9/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0379 - val_loss: 0.0384\n",
      "Epoch 10/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0372 - val_loss: 0.0355\n",
      "Epoch 11/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0358 - val_loss: 0.0360\n",
      "Epoch 12/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0347 - val_loss: 0.0328\n",
      "Epoch 13/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0333 - val_loss: 0.0339\n",
      "Epoch 14/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0330 - val_loss: 0.0322\n",
      "Epoch 15/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0323 - val_loss: 0.0314\n",
      "Epoch 16/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0318 - val_loss: 0.0291\n",
      "Epoch 17/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0314 - val_loss: 0.0331\n",
      "Epoch 18/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0312 - val_loss: 0.0342\n",
      "Epoch 19/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0298 - val_loss: 0.0283\n",
      "Epoch 20/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0297 - val_loss: 0.0313\n",
      "Epoch 21/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0291 - val_loss: 0.0301\n",
      "Epoch 22/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0290 - val_loss: 0.0292\n",
      "Epoch 23/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0287 - val_loss: 0.0316\n",
      "Epoch 24/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0286 - val_loss: 0.0269\n",
      "Epoch 25/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0282 - val_loss: 0.0284\n",
      "Epoch 26/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0275 - val_loss: 0.0295\n",
      "Epoch 27/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0272 - val_loss: 0.0299\n",
      "Epoch 28/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0268 - val_loss: 0.0276\n",
      "Epoch 29/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0273 - val_loss: 0.0274\n",
      "Epoch 30/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0268 - val_loss: 0.0281\n",
      "Epoch 31/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0264 - val_loss: 0.0263\n",
      "Epoch 32/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0264 - val_loss: 0.0270\n",
      "Epoch 33/300\n",
      "91300/91300 [==============================] - 2s 24us/step - loss: 0.0261 - val_loss: 0.0252\n",
      "Epoch 34/300\n",
      "91300/91300 [==============================] - 3s 31us/step - loss: 0.0257 - val_loss: 0.0245\n",
      "Epoch 35/300\n",
      "91300/91300 [==============================] - 3s 30us/step - loss: 0.0259 - val_loss: 0.0251\n",
      "Epoch 36/300\n",
      "91300/91300 [==============================] - 2s 24us/step - loss: 0.0254 - val_loss: 0.0298\n",
      "Epoch 37/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0253 - val_loss: 0.0257\n",
      "Epoch 38/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0254 - val_loss: 0.0242\n",
      "Epoch 39/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0250 - val_loss: 0.0236\n",
      "Epoch 40/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0248 - val_loss: 0.0238\n",
      "Epoch 41/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0249 - val_loss: 0.0247\n",
      "Epoch 42/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0248 - val_loss: 0.0240\n",
      "Epoch 43/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0248 - val_loss: 0.0219\n",
      "Epoch 44/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0247 - val_loss: 0.0251\n",
      "Epoch 45/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0242 - val_loss: 0.0256\n",
      "Epoch 46/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0239 - val_loss: 0.0246\n",
      "Epoch 47/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0239 - val_loss: 0.0225\n",
      "Epoch 48/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0242 - val_loss: 0.0246\n",
      "Epoch 49/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0238 - val_loss: 0.0234\n",
      "Epoch 50/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0235 - val_loss: 0.0270\n",
      "Epoch 51/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0236 - val_loss: 0.0239\n",
      "Epoch 52/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0235 - val_loss: 0.0258\n",
      "Epoch 53/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0234 - val_loss: 0.0271\n",
      "Epoch 54/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0241 - val_loss: 0.0237\n",
      "Epoch 55/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0235 - val_loss: 0.0270\n",
      "Epoch 56/300\n",
      "91300/91300 [==============================] - 4s 49us/step - loss: 0.0229 - val_loss: 0.0240\n",
      "Epoch 57/300\n",
      "91300/91300 [==============================] - 3s 38us/step - loss: 0.0232 - val_loss: 0.0227\n",
      "Epoch 58/300\n",
      "91300/91300 [==============================] - 2s 19us/step - loss: 0.0228 - val_loss: 0.0232\n",
      "Epoch 59/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0233 - val_loss: 0.0217\n",
      "Epoch 60/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0231 - val_loss: 0.0245\n",
      "Epoch 61/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0226 - val_loss: 0.0261\n",
      "Epoch 62/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0229 - val_loss: 0.0206\n",
      "Epoch 63/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0226 - val_loss: 0.0221\n",
      "Epoch 64/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0229 - val_loss: 0.0233\n",
      "Epoch 65/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0230 - val_loss: 0.0218\n",
      "Epoch 66/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0230 - val_loss: 0.0212\n",
      "Epoch 67/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0224 - val_loss: 0.0258\n",
      "Epoch 68/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0224 - val_loss: 0.0259\n",
      "Epoch 69/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0227 - val_loss: 0.0244\n",
      "Epoch 70/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0227 - val_loss: 0.0226\n",
      "Epoch 71/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0223 - val_loss: 0.0238\n",
      "Epoch 72/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0226 - val_loss: 0.0201\n",
      "Epoch 73/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0224 - val_loss: 0.0261\n",
      "Epoch 74/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0222 - val_loss: 0.0202\n",
      "Epoch 75/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0223 - val_loss: 0.0201\n",
      "Epoch 76/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0218 - val_loss: 0.0235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0220 - val_loss: 0.0211\n",
      "Epoch 78/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0217 - val_loss: 0.0209\n",
      "Epoch 79/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0220 - val_loss: 0.0213\n",
      "Epoch 80/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0216 - val_loss: 0.0264\n",
      "Epoch 81/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0217 - val_loss: 0.0208\n",
      "Epoch 82/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0215 - val_loss: 0.0240\n",
      "Epoch 83/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0218 - val_loss: 0.0248\n",
      "Epoch 84/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0214 - val_loss: 0.0218\n",
      "Epoch 85/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0217 - val_loss: 0.0249\n",
      "Epoch 86/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0213 - val_loss: 0.0256\n",
      "Epoch 87/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0214 - val_loss: 0.0207\n",
      "Epoch 88/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0211 - val_loss: 0.0225\n",
      "Epoch 89/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0211 - val_loss: 0.0200\n",
      "Epoch 90/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0214 - val_loss: 0.0220\n",
      "Epoch 91/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0214 - val_loss: 0.0221\n",
      "Epoch 92/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0210 - val_loss: 0.0209\n",
      "Epoch 93/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0214 - val_loss: 0.0207\n",
      "Epoch 94/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0213 - val_loss: 0.0213\n",
      "Epoch 95/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0205 - val_loss: 0.0224\n",
      "Epoch 96/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0209 - val_loss: 0.0227\n",
      "Epoch 97/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0209 - val_loss: 0.0200\n",
      "Epoch 98/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0209 - val_loss: 0.0237\n",
      "Epoch 99/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0209 - val_loss: 0.0194\n",
      "Epoch 100/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0211 - val_loss: 0.0230\n",
      "Epoch 101/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0207 - val_loss: 0.0219\n",
      "Epoch 102/300\n",
      "91300/91300 [==============================] - 2s 26us/step - loss: 0.0209 - val_loss: 0.0209\n",
      "Epoch 103/300\n",
      "91300/91300 [==============================] - 2s 24us/step - loss: 0.0210 - val_loss: 0.0212\n",
      "Epoch 104/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0204 - val_loss: 0.0222\n",
      "Epoch 105/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0210 - val_loss: 0.0210: 0\n",
      "Epoch 106/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0205 - val_loss: 0.0211\n",
      "Epoch 107/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0205 - val_loss: 0.0225\n",
      "Epoch 108/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0211 - val_loss: 0.0233\n",
      "Epoch 109/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0203 - val_loss: 0.0214\n",
      "Epoch 110/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0204 - val_loss: 0.0208\n",
      "Epoch 111/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0205 - val_loss: 0.0185\n",
      "Epoch 112/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0206 - val_loss: 0.0195\n",
      "Epoch 113/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0206 - val_loss: 0.0225\n",
      "Epoch 114/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0204 - val_loss: 0.0219\n",
      "Epoch 115/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0205 - val_loss: 0.0205\n",
      "Epoch 116/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0208 - val_loss: 0.0225\n",
      "Epoch 117/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0201 - val_loss: 0.0190\n",
      "Epoch 118/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0201 - val_loss: 0.0221\n",
      "Epoch 119/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0202 - val_loss: 0.0221\n",
      "Epoch 120/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0205 - val_loss: 0.0200\n",
      "Epoch 121/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0207 - val_loss: 0.0220\n",
      "Epoch 122/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0203 - val_loss: 0.0252\n",
      "Epoch 123/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0202 - val_loss: 0.0206\n",
      "Epoch 124/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0201 - val_loss: 0.0209\n",
      "Epoch 125/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0200 - val_loss: 0.0212\n",
      "Epoch 126/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0202 - val_loss: 0.0201\n",
      "Epoch 127/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0202 - val_loss: 0.0185\n",
      "Epoch 128/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0200 - val_loss: 0.0202\n",
      "Epoch 129/300\n",
      "91300/91300 [==============================] - 2s 24us/step - loss: 0.0198 - val_loss: 0.0229\n",
      "Epoch 130/300\n",
      "91300/91300 [==============================] - 2s 24us/step - loss: 0.0200 - val_loss: 0.0207\n",
      "Epoch 131/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0201 - val_loss: 0.0230\n",
      "Epoch 132/300\n",
      "91300/91300 [==============================] - 2s 26us/step - loss: 0.0199 - val_loss: 0.0209\n",
      "Epoch 133/300\n",
      "91300/91300 [==============================] - 2s 26us/step - loss: 0.0197 - val_loss: 0.0189\n",
      "Epoch 134/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0199 - val_loss: 0.0202\n",
      "Epoch 135/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0197 - val_loss: 0.0202\n",
      "Epoch 136/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0198 - val_loss: 0.0209\n",
      "Epoch 137/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0201 - val_loss: 0.0211\n",
      "Epoch 138/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0201 - val_loss: 0.0214\n",
      "Epoch 139/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0204 - val_loss: 0.0217\n",
      "Epoch 140/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0201 - val_loss: 0.0200\n",
      "Epoch 141/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0198 - val_loss: 0.0188\n",
      "Epoch 142/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0200 - val_loss: 0.0206\n",
      "Epoch 143/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0196 - val_loss: 0.0218\n",
      "Epoch 144/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0196 - val_loss: 0.0231\n",
      "Epoch 145/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0195 - val_loss: 0.0222\n",
      "Epoch 146/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0201 - val_loss: 0.0196\n",
      "Epoch 147/300\n",
      "91300/91300 [==============================] - 2s 25us/step - loss: 0.0197 - val_loss: 0.0221\n",
      "Epoch 148/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0196 - val_loss: 0.0202\n",
      "Epoch 149/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0197 - val_loss: 0.0204\n",
      "Epoch 150/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0199 - val_loss: 0.0231\n",
      "Epoch 151/300\n",
      "91300/91300 [==============================] - 2s 25us/step - loss: 0.0198 - val_loss: 0.0230\n",
      "Epoch 152/300\n",
      "91300/91300 [==============================] - 2s 27us/step - loss: 0.0195 - val_loss: 0.0227\n",
      "Epoch 153/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91300/91300 [==============================] - 4s 39us/step - loss: 0.0199 - val_loss: 0.0194\n",
      "Epoch 154/300\n",
      "91300/91300 [==============================] - 3s 36us/step - loss: 0.0196 - val_loss: 0.0195\n",
      "Epoch 155/300\n",
      "91300/91300 [==============================] - 4s 41us/step - loss: 0.0196 - val_loss: 0.0197\n",
      "Epoch 156/300\n",
      "91300/91300 [==============================] - 2s 27us/step - loss: 0.0195 - val_loss: 0.0214\n",
      "Epoch 157/300\n",
      "91300/91300 [==============================] - 2s 25us/step - loss: 0.0196 - val_loss: 0.0205\n",
      "Epoch 158/300\n",
      "91300/91300 [==============================] - 3s 29us/step - loss: 0.0194 - val_loss: 0.0220\n",
      "Epoch 159/300\n",
      "91300/91300 [==============================] - 2s 26us/step - loss: 0.0193 - val_loss: 0.0200\n",
      "Epoch 160/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0194 - val_loss: 0.0215\n",
      "Epoch 161/300\n",
      "91300/91300 [==============================] - 2s 26us/step - loss: 0.0192 - val_loss: 0.0185\n",
      "Epoch 162/300\n",
      "91300/91300 [==============================] - 2s 25us/step - loss: 0.0192 - val_loss: 0.0201\n",
      "Epoch 163/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0194 - val_loss: 0.0204\n",
      "Epoch 164/300\n",
      "91300/91300 [==============================] - 3s 38us/step - loss: 0.0197 - val_loss: 0.0202\n",
      "Epoch 165/300\n",
      "91300/91300 [==============================] - 3s 34us/step - loss: 0.0190 - val_loss: 0.0229\n",
      "Epoch 166/300\n",
      "91300/91300 [==============================] - 3s 28us/step - loss: 0.0194 - val_loss: 0.0209\n",
      "Epoch 167/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0194 - val_loss: 0.0217\n",
      "Epoch 168/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0200 - val_loss: 0.0195\n",
      "Epoch 169/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0190 - val_loss: 0.0207\n",
      "Epoch 170/300\n",
      "91300/91300 [==============================] - 2s 26us/step - loss: 0.0193 - val_loss: 0.0222\n",
      "Epoch 171/300\n",
      "91300/91300 [==============================] - 3s 30us/step - loss: 0.0192 - val_loss: 0.0213\n",
      "Epoch 172/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0195 - val_loss: 0.0217\n",
      "Epoch 173/300\n",
      "91300/91300 [==============================] - 2s 26us/step - loss: 0.0195 - val_loss: 0.0184\n",
      "Epoch 174/300\n",
      "91300/91300 [==============================] - 3s 31us/step - loss: 0.0193 - val_loss: 0.0219\n",
      "Epoch 175/300\n",
      "91300/91300 [==============================] - 3s 30us/step - loss: 0.0188 - val_loss: 0.0198\n",
      "Epoch 176/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0193 - val_loss: 0.0203\n",
      "Epoch 177/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0192 - val_loss: 0.0181\n",
      "Epoch 178/300\n",
      "91300/91300 [==============================] - 2s 25us/step - loss: 0.0189 - val_loss: 0.0206\n",
      "Epoch 179/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0191 - val_loss: 0.0179\n",
      "Epoch 180/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0193 - val_loss: 0.0198\n",
      "Epoch 181/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0190 - val_loss: 0.0231\n",
      "Epoch 182/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0195 - val_loss: 0.0190\n",
      "Epoch 183/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0194 - val_loss: 0.0197\n",
      "Epoch 184/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0191 - val_loss: 0.0196\n",
      "Epoch 185/300\n",
      "91300/91300 [==============================] - 2s 24us/step - loss: 0.0190 - val_loss: 0.0217\n",
      "Epoch 186/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0192 - val_loss: 0.0190\n",
      "Epoch 187/300\n",
      "91300/91300 [==============================] - 3s 33us/step - loss: 0.0194 - val_loss: 0.0199\n",
      "Epoch 188/300\n",
      "91300/91300 [==============================] - 2s 24us/step - loss: 0.0188 - val_loss: 0.0203\n",
      "Epoch 189/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0189 - val_loss: 0.0186\n",
      "Epoch 190/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0192 - val_loss: 0.0192\n",
      "Epoch 191/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0188 - val_loss: 0.0194\n",
      "Epoch 192/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0192 - val_loss: 0.0204\n",
      "Epoch 193/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0190 - val_loss: 0.0202\n",
      "Epoch 194/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0191 - val_loss: 0.0205\n",
      "Epoch 195/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0186 - val_loss: 0.0197\n",
      "Epoch 196/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0186 - val_loss: 0.0223\n",
      "Epoch 197/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0192 - val_loss: 0.0205\n",
      "Epoch 198/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0193 - val_loss: 0.0215\n",
      "Epoch 199/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0187 - val_loss: 0.0193\n",
      "Epoch 200/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0189 - val_loss: 0.0235\n",
      "Epoch 201/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0188 - val_loss: 0.0187\n",
      "Epoch 202/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0188 - val_loss: 0.0204\n",
      "Epoch 203/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0190 - val_loss: 0.0190\n",
      "Epoch 204/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0190 - val_loss: 0.0196\n",
      "Epoch 205/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0188 - val_loss: 0.0206\n",
      "Epoch 206/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0184 - val_loss: 0.0199\n",
      "Epoch 207/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0190 - val_loss: 0.0182\n",
      "Epoch 208/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0185 - val_loss: 0.0182\n",
      "Epoch 209/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0189 - val_loss: 0.0177\n",
      "Epoch 210/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0187 - val_loss: 0.0185\n",
      "Epoch 211/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0185 - val_loss: 0.0192\n",
      "Epoch 212/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0185 - val_loss: 0.0188\n",
      "Epoch 213/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0187 - val_loss: 0.0193\n",
      "Epoch 214/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0185 - val_loss: 0.0179\n",
      "Epoch 215/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0189 - val_loss: 0.0191\n",
      "Epoch 216/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0184 - val_loss: 0.0221\n",
      "Epoch 217/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0184 - val_loss: 0.0201\n",
      "Epoch 218/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0188 - val_loss: 0.0192\n",
      "Epoch 219/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0188 - val_loss: 0.0188\n",
      "Epoch 220/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0184 - val_loss: 0.0209\n",
      "Epoch 221/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0186 - val_loss: 0.0181\n",
      "Epoch 222/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0186 - val_loss: 0.0190\n",
      "Epoch 223/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0185 - val_loss: 0.0204\n",
      "Epoch 224/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0186 - val_loss: 0.0215\n",
      "Epoch 225/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0185 - val_loss: 0.0199\n",
      "Epoch 226/300\n",
      "91300/91300 [==============================] - 3s 28us/step - loss: 0.0185 - val_loss: 0.0189\n",
      "Epoch 227/300\n",
      "91300/91300 [==============================] - 2s 23us/step - loss: 0.0184 - val_loss: 0.0188\n",
      "Epoch 228/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0185 - val_loss: 0.0222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0185 - val_loss: 0.0190\n",
      "Epoch 230/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0184 - val_loss: 0.0183\n",
      "Epoch 231/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0184 - val_loss: 0.0187\n",
      "Epoch 232/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0185 - val_loss: 0.0186\n",
      "Epoch 233/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0188 - val_loss: 0.0203\n",
      "Epoch 234/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0186 - val_loss: 0.0224\n",
      "Epoch 235/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0181 - val_loss: 0.0218\n",
      "Epoch 236/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0186 - val_loss: 0.0179\n",
      "Epoch 237/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0184 - val_loss: 0.0187\n",
      "Epoch 238/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0187 - val_loss: 0.0205\n",
      "Epoch 239/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0183 - val_loss: 0.0204\n",
      "Epoch 240/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0184 - val_loss: 0.0210\n",
      "Epoch 241/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0185 - val_loss: 0.0209\n",
      "Epoch 242/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0184 - val_loss: 0.0187\n",
      "Epoch 243/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0185 - val_loss: 0.0198\n",
      "Epoch 244/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0183 - val_loss: 0.0176\n",
      "Epoch 245/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0182 - val_loss: 0.0178\n",
      "Epoch 246/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0185 - val_loss: 0.0202\n",
      "Epoch 247/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0181 - val_loss: 0.0180\n",
      "Epoch 248/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0183 - val_loss: 0.0201\n",
      "Epoch 249/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0182 - val_loss: 0.0183\n",
      "Epoch 250/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0180 - val_loss: 0.0188\n",
      "Epoch 251/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0182 - val_loss: 0.0202\n",
      "Epoch 252/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0182 - val_loss: 0.0180\n",
      "Epoch 253/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0184 - val_loss: 0.0201\n",
      "Epoch 254/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0184 - val_loss: 0.0190\n",
      "Epoch 255/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0179 - val_loss: 0.0179\n",
      "Epoch 256/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0182 - val_loss: 0.0186\n",
      "Epoch 257/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0183 - val_loss: 0.0201\n",
      "Epoch 258/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0184 - val_loss: 0.0169\n",
      "Epoch 259/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0183 - val_loss: 0.0182\n",
      "Epoch 260/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0180 - val_loss: 0.0190\n",
      "Epoch 261/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0178 - val_loss: 0.0212\n",
      "Epoch 262/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0183 - val_loss: 0.0196\n",
      "Epoch 263/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0183 - val_loss: 0.0201\n",
      "Epoch 264/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0180 - val_loss: 0.0174\n",
      "Epoch 265/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0181 - val_loss: 0.0185\n",
      "Epoch 266/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0182 - val_loss: 0.0180\n",
      "Epoch 267/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0181 - val_loss: 0.0186\n",
      "Epoch 268/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0179 - val_loss: 0.0219\n",
      "Epoch 269/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0182 - val_loss: 0.0187\n",
      "Epoch 270/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0182 - val_loss: 0.0199\n",
      "Epoch 271/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0182 - val_loss: 0.0181\n",
      "Epoch 272/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0181 - val_loss: 0.0188\n",
      "Epoch 273/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0183 - val_loss: 0.0230\n",
      "Epoch 274/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0181 - val_loss: 0.0198\n",
      "Epoch 275/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0181 - val_loss: 0.0184\n",
      "Epoch 276/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0179 - val_loss: 0.0188\n",
      "Epoch 277/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0183 - val_loss: 0.0180\n",
      "Epoch 278/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0182 - val_loss: 0.0206\n",
      "Epoch 279/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0182 - val_loss: 0.0188\n",
      "Epoch 280/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0178 - val_loss: 0.0205\n",
      "Epoch 281/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0181 - val_loss: 0.0178\n",
      "Epoch 282/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0178 - val_loss: 0.0190\n",
      "Epoch 283/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0178 - val_loss: 0.0187\n",
      "Epoch 284/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0180 - val_loss: 0.0180\n",
      "Epoch 285/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0180 - val_loss: 0.0190\n",
      "Epoch 286/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0180 - val_loss: 0.0175\n",
      "Epoch 287/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0180 - val_loss: 0.0201\n",
      "Epoch 288/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0180 - val_loss: 0.0189\n",
      "Epoch 289/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0180 - val_loss: 0.0172\n",
      "Epoch 290/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0178 - val_loss: 0.0173\n",
      "Epoch 291/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0180 - val_loss: 0.0201\n",
      "Epoch 292/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0179 - val_loss: 0.0177\n",
      "Epoch 293/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0181 - val_loss: 0.0183\n",
      "Epoch 294/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0180 - val_loss: 0.0189\n",
      "Epoch 295/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0178 - val_loss: 0.0189\n",
      "Epoch 296/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0182 - val_loss: 0.0194\n",
      "Epoch 297/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0178 - val_loss: 0.0189\n",
      "Epoch 298/300\n",
      "91300/91300 [==============================] - 2s 20us/step - loss: 0.0177 - val_loss: 0.0192\n",
      "Epoch 299/300\n",
      "91300/91300 [==============================] - 2s 22us/step - loss: 0.0178 - val_loss: 0.0192\n",
      "Epoch 300/300\n",
      "91300/91300 [==============================] - 2s 21us/step - loss: 0.0178 - val_loss: 0.0182\n"
     ]
    }
   ],
   "source": [
    "cb =[EarlyStopping(monitor='val_loss', patience=patience, verbose =1, restore_best_weights=True)]\n",
    "history = model.fit(x_train, y_train,validation_data=(x_val, y_val),epochs=epochs, batch_size=batch_size, verbose=1, callbacks= cb)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Plot Training Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('training_curves_paper.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Predict Position "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test, batch_size=batch_size) \n",
    "y_predict_in_val = model.predict(x_val, batch_size=batch_size)\n",
    "y_predict_in_train = model.predict(x_train, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revert the Representation from normalize to lat-long coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = scaler_y.inverse_transform(y_predict)\n",
    "y_predict_in_train = scaler_y.inverse_transform(y_predict_in_train)\n",
    "y_predict_in_val = scaler_y.inverse_transform(y_predict_in_val)\n",
    "y_train = scaler_y.inverse_transform(y_train)\n",
    "y_val = scaler_y.inverse_transform(y_val)\n",
    "y_test = scaler_y.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Haversine Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set mean error: 176.70\n",
      "Train set median error: 87.29\n",
      "Train set75th perc error: 196.33\n",
      "Val set mean error: 189.67\n",
      "Val set median error: 93.53\n",
      "Val set 75th perc.  error: 222.85\n",
      "Test set mean error: 190.88\n",
      "Test set median error: 92.68\n",
      "Test set  75th perc. error: 219.55\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set mean error: {:.2f}\".format(my_custom_haversine_error_stats(y_predict_in_train, y_train,'mean')))\n",
    "print(\"Train set median error: {:.2f}\".format(my_custom_haversine_error_stats(y_predict_in_train, y_train,'median')))\n",
    "print(\"Train set75th perc error: {:.2f}\".format(my_custom_haversine_error_stats(y_predict_in_train, y_train,'percentile',75)))\n",
    "print(\"Val set mean error: {:.2f}\".format(my_custom_haversine_error_stats(y_predict_in_val, y_val,'mean')))\n",
    "print(\"Val set median error: {:.2f}\".format(my_custom_haversine_error_stats(y_predict_in_val, y_val,'median')))\n",
    "print(\"Val set 75th perc.  error: {:.2f}\".format(my_custom_haversine_error_stats(y_predict_in_val, y_val,'percentile',75)))\n",
    "print(\"Test set mean error: {:.2f}\".format(my_custom_haversine_error_stats(y_predict, y_test,'mean')))\n",
    "print(\"Test set median error: {:.2f}\".format(my_custom_haversine_error_stats(y_predict, y_test,'median')))\n",
    "print(\"Test set  75th perc. error: {:.2f}\".format(my_custom_haversine_error_stats(y_predict, y_test,'percentile',75)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment completed!!!\n"
     ]
    }
   ],
   "source": [
    "test_error_list = calculate_pairwise_error_list(y_predict,y_test)\n",
    "p.DataFrame(test_error_list).to_csv(\"mlp_test_error_list.csv\")\n",
    "print(\"Experiment completed!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras library import  for Saving and loading model and weights\n",
    "\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "\n",
    "# serialize model to JSON\n",
    "#  the keras model which is trained is defined as 'model' in this example\n",
    "model_json = model.to_json()\n",
    "\n",
    "\n",
    "with open(\"baseline_paper.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"baseline_paper.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
